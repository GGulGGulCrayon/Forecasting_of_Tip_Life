{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 해당쿼리로 데이터 추출\n",
    "\n",
    "SELECT datetime, shot_no, tool_number, current_spindle, current_x, current_z, vibration, rpm, rpm_set, feed, feed_set, load_1, servo_load_x, servo_load_z, servo_current_x, servo_current_z from TPOP_MACHINE_PARAMETER\n",
    "WHERE mc_id = 22 and date between '2022-07-13' AND '2022-10-12'\n",
    "\n",
    "- 날짜 기준 범위는 1개월 반씩 분리해서 Query 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook module.py to python\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jhpark\\anaconda3\\lib\\site-packages\\nbformat\\reader.py\", line 14, in parse_json\n",
      "    nb_dict = json.loads(s, **kwargs)\n",
      "  File \"C:\\Users\\jhpark\\anaconda3\\lib\\json\\__init__.py\", line 357, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"C:\\Users\\jhpark\\anaconda3\\lib\\json\\decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"C:\\Users\\jhpark\\anaconda3\\lib\\json\\decoder.py\", line 355, in raw_decode\n",
      "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
      "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jhpark\\anaconda3\\Scripts\\jupyter-nbconvert-script.py\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"C:\\Users\\jhpark\\anaconda3\\lib\\site-packages\\jupyter_core\\application.py\", line 270, in launch_instance\n",
      "    return super(JupyterApp, cls).launch_instance(argv=argv, **kwargs)\n",
      "  File \"C:\\Users\\jhpark\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 845, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\jhpark\\anaconda3\\lib\\site-packages\\nbconvert\\nbconvertapp.py\", line 350, in start\n",
      "    self.convert_notebooks()\n",
      "  File \"C:\\Users\\jhpark\\anaconda3\\lib\\site-packages\\nbconvert\\nbconvertapp.py\", line 524, in convert_notebooks\n",
      "    self.convert_single_notebook(notebook_filename)\n",
      "  File \"C:\\Users\\jhpark\\anaconda3\\lib\\site-packages\\nbconvert\\nbconvertapp.py\", line 489, in convert_single_notebook\n",
      "    output, resources = self.export_single_notebook(notebook_filename, resources, input_buffer=input_buffer)\n",
      "  File \"C:\\Users\\jhpark\\anaconda3\\lib\\site-packages\\nbconvert\\nbconvertapp.py\", line 418, in export_single_notebook\n",
      "    output, resources = self.exporter.from_filename(notebook_filename, resources=resources)\n",
      "  File \"C:\\Users\\jhpark\\anaconda3\\lib\\site-packages\\nbconvert\\exporters\\exporter.py\", line 181, in from_filename\n",
      "    return self.from_file(f, resources=resources, **kw)\n",
      "  File \"C:\\Users\\jhpark\\anaconda3\\lib\\site-packages\\nbconvert\\exporters\\exporter.py\", line 199, in from_file\n",
      "    return self.from_notebook_node(nbformat.read(file_stream, as_version=4), resources=resources, **kw)\n",
      "  File \"C:\\Users\\jhpark\\anaconda3\\lib\\site-packages\\nbformat\\__init__.py\", line 143, in read\n",
      "    return reads(buf, as_version, **kwargs)\n",
      "  File \"C:\\Users\\jhpark\\anaconda3\\lib\\site-packages\\nbformat\\__init__.py\", line 73, in reads\n",
      "    nb = reader.reads(s, **kwargs)\n",
      "  File \"C:\\Users\\jhpark\\anaconda3\\lib\\site-packages\\nbformat\\reader.py\", line 58, in reads\n",
      "    nb_dict = parse_json(s, **kwargs)\n",
      "  File \"C:\\Users\\jhpark\\anaconda3\\lib\\site-packages\\nbformat\\reader.py\", line 17, in parse_json\n",
      "    raise NotJSONError((\"Notebook does not appear to be JSON: %r\" % s)[:77] + \"...\") from e\n",
      "nbformat.reader.NotJSONError: Notebook does not appear to be JSON: '#!/usr/bin/env python\\n# coding: utf-8\\...\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to python module.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-05T23:56:52.628338Z",
     "start_time": "2022-10-05T23:56:48.003944Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import random\n",
    "from sklearn.ensemble import IsolationForest\n",
    "warnings.filterwarnings('ignore')\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-05T23:56:52.643331Z",
     "start_time": "2022-10-05T23:56:52.630308Z"
    }
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_row', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 함수 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 일반"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 컬럼 데이터 형변환 (64bit → 32bit)\n",
    " * 데이터 용량 줄이기 위한 용도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-05T23:56:52.658332Z",
     "start_time": "2022-10-05T23:56:52.645310Z"
    }
   },
   "outputs": [],
   "source": [
    "#주어진 dataframe에서 데이터 타입이 64bit인 경우 32bit로 변경하여 데이터 용량 축소\n",
    "def change_data_type_64bit_to_32bit(data):\n",
    "    column_names = list(data.columns.values)\n",
    "    \n",
    "    for col in column_names:\n",
    "        data_type = str(data[col].dtype)\n",
    "        \n",
    "        if data_type == 'int64':\n",
    "            data[col] = data[col].astype('int32')\n",
    "        elif data_type == 'float64':\n",
    "            data[col] = data[col].astype('float32')\n",
    "        #LJY_20220929 : 컬럼의 data type이 'object'인데 실제 값은 numeric인 경우 int32로 변환\n",
    "        # 추후 데이터 타입 판별로 변경 필요 (numeric인지 체크하는 api에서 datetime을 걸러내지 못해서 임시로 column name으로 체크하도록 처리)\n",
    "        else:\n",
    "            if col != \"datetime\":\n",
    "                data[col] = data[col].replace('\\\\0', 0)\n",
    "                data[col] = data[col].fillna(0).astype('int32')\n",
    "#                data[col] = (data[col].fillna(0).astype('int32').astype(object).where(data[col].notnull()))\n",
    "#                data[col] = data[col].notnull().astype('int32')\n",
    "    \n",
    "    column_names.clear()\n",
    "    del column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tool 정보 추출\n",
    " * 'tool_number' 필드에서 공구 정보를 추출하여 별도의 컬럼으로 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-05T23:56:52.673307Z",
     "start_time": "2022-10-05T23:56:52.660308Z"
    }
   },
   "outputs": [],
   "source": [
    "# tool 번호 및 상태 생성\n",
    "def extract_tool_info_from_data(data):\n",
    "    if 'tool_number' not in data.columns:\n",
    "        return\n",
    "    \n",
    "    data['tool_state'] = data['tool_number']%100\n",
    "    data['tool'] = (data['tool_number'] - data['tool_state'])/100\n",
    "    if data['tool'].isnull().sum() > 0:\n",
    "        data = data.dropna(axis = 0)\n",
    "    data['tool'] = data['tool'].astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주어진 경로의 csv 파일 목록 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-05T23:56:52.688308Z",
     "start_time": "2022-10-05T23:56:52.675332Z"
    }
   },
   "outputs": [],
   "source": [
    "#주어진 경로의 .csv파일을 찾아서 list 형태로 반환\n",
    "def find_csv_files(path):\n",
    "    file_list = os.listdir(path)\n",
    "    \n",
    "    file_list_csv = [file_csv for file_csv in file_list if file_csv.endswith('.csv')]\n",
    "    file_list_csv\n",
    "    \n",
    "    file_list.clear()\n",
    "    del file_list\n",
    "    \n",
    "    return file_list_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주어진 경로의 대상 csv파일을 불러오기\n",
    " * 입력인자 중 sensor_data_only : True이면 전류 센서 데이터만 포함, False이면 Focas 데이터를 포함한 파일을 의미\n",
    " * 데이터 종류를 입력으로 하여 해당하는 데이터 타입의 .csv파일 Load\n",
    " * 데이터 타입 형변환 (64bit → 32bit, object타입의 경우 datetime 컬럼을 제외하고 int32로 변환)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-05T23:56:52.703308Z",
     "start_time": "2022-10-05T23:56:52.690309Z"
    }
   },
   "outputs": [],
   "source": [
    "#주어진 경로의 .csv파일을 찾아서 모두 병합한 dataframe 구성\n",
    "def build_dataframe_from_cvs_files(path, sensor_data_only=True, display_report=False):\n",
    "    file_list_csv = find_csv_files(path)\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    if len(file_list_csv) < 1:\n",
    "        return df\n",
    "\n",
    "    #전류 센서 & focas 데이터 포함 파일의 header부분(컬럼명 나열된 line) 정보\n",
    "    all_data_columns = '\"datetime\",\"shot_no\",\"tool_number\",\"current_spindle\",\"current_x\",\"current_z\",\"vibration\",\"rpm\",\"rpm_set\",\"feed\",\"feed_set\",\"load_1\",\"servo_load_x\",\"servo_load_z\",\"servo_current_x\",\"servo_current_z\"'\n",
    "    # all_data_columns = ',datetime,tool_number,shot_no,current_x,current_z,current_spindle,rpm,feed,load_1,servo_load_x,servo_load_z,servo_current_x,servo_current_z'\n",
    "\n",
    "    \n",
    "    #전류 센서데이터만 포함한 파일의 header부분(컬럼명 나열된 line) 정보\n",
    "    sensor_data_only_columns = '\"datetime\",\"tool_number\",\"shot_no\",\"current_spindle\",\"current_x\",\"current_z\"'\n",
    "    \n",
    "    for csv_idx in file_list_csv:\n",
    "        first_line = \"\"\n",
    "        \n",
    "        #encoding을 utf-8-sig로 해야 파일에서 읽은 문자열의 맨 앞에 '\\ufeff'가 붙는 현상을 막을 수 있음\n",
    "        # - utf-8로 해서 읽으면 맨 앞에 '\\ufeff'가 붙어서 위에 정의한 all_data_columns, sensor_data_only_columns와 비교 시 무조건 False 발생\n",
    "        with open(csv_idx, encoding=\"utf-8-sig\") as f:\n",
    "            first_line = f.readline()\n",
    "            print(first_line)\n",
    "            first_line = first_line.strip('\\n')\n",
    "            f.close()\n",
    "        \n",
    "        if display_report is True:\n",
    "            print(first_line)\n",
    "        \n",
    "        #전류 센서 & focas 데이터 포함 파일 (무의미한 column은 DB에서 조회할 때 제외하고 조회 완료)\n",
    "        if first_line == all_data_columns:\n",
    "            if display_report is True:\n",
    "                print(\"file {} : all data\".format(csv_idx))\n",
    "                \n",
    "            if sensor_data_only is True:\n",
    "                continue\n",
    "        #전류 센서 데이터만 포함한 파일\n",
    "        elif first_line == sensor_data_only_columns:\n",
    "            if display_report is True:\n",
    "                print(\"file {} : current sensor data only\".format(csv_idx))\n",
    "                \n",
    "            if sensor_data_only is False:\n",
    "                continue\n",
    "        else:\n",
    "            if display_report is True:\n",
    "                print(\"file {} : invalid data\".format(csv_idx))\n",
    "                \n",
    "            continue\n",
    "            \n",
    "        csv_df = pd.read_csv(csv_idx)\n",
    "            \n",
    "        #데이터 타입을 64bit → 32bit로 변경하여 용량 축소\n",
    "        change_data_type_64bit_to_32bit(csv_df)\n",
    "    \n",
    "        #print(csv_df.info())\n",
    "\n",
    "        df = pd.concat([df, csv_df], axis = 0)\n",
    "        del csv_df\n",
    "    \n",
    "    file_list_csv.clear()\n",
    "    del file_list_csv\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주어진 경로의 대상 파일을 불러오기\n",
    " * 입력인자 중 sensor_data_only : True이면 전류 센서 데이터만 포함, False이면 Focas 데이터를 포함한 파일을 의미\n",
    " * current directory에서 .parquet파일을 찾아서 불러오기 \n",
    " * .parquet파일이 없는 경우 .csv를 읽어서 통합한 후 .parquet로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-05T23:56:56.964825Z",
     "start_time": "2022-10-05T23:56:52.705309Z"
    }
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------\n",
    "#☆☆ Parquet(파케이)란?\n",
    "#   - Apache Parquet은 쿼리 속도를 높이기 위한 최적화를 제공하는 열 형식 파일 형식이며 CSV 또는 JSON보다 훨씬 효율적인 파일 형식입니다.\n",
    "#   - <참고> : https://pearlluck.tistory.com/561\n",
    "#--------------------------------------------------------------\n",
    "#\n",
    "#\n",
    "#Parquet(파케이) 파일이 있는 경우 해당 파일을 읽고, 그렇지 않은 경우 .csv 파일을 순회하면서 읽어서 merge\n",
    "#pyarrow 모듈 설치해야 .parquet 파일 입출력 기능 사용 가능\n",
    "# !pip install pyarrow\n",
    "\n",
    "def read_data(sensor_data_only=True, extract_tool_info=True, display_report=False):\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    #전류 센서로만 구성된 데이터를 사용하려고 하는 경우\n",
    "    if sensor_data_only is True:\n",
    "        if os.path.isfile('./VL04_data_sensor_only.parquet'): #파일이 있는 경우\n",
    "            df = pd.read_parquet('./VL04_data_sensor_only.parquet')  #.parquet 데이터 ()\n",
    "    #focas 데이터까지 포함된 데이터를 사용하려고 하는 경우\n",
    "    else:    \n",
    "        if os.path.isfile('./VL04_data.parquet'): #파일이 있는 경우\n",
    "            df = pd.read_parquet('./VL04_data.parquet')  #.parquet 데이터 ()\n",
    "    \n",
    "    if df.empty:\n",
    "        #현재 디렉토리의 .csv파일을 모두 읽어서 하나의 dataframe으로 구성\n",
    "        df = build_dataframe_from_cvs_files('./', sensor_data_only, display_report)\n",
    "\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "        #dataframe을 .pqrquet 파일로 저장하여 추후 상대적으로 적은 메모리의 파일을 빠르게 읽어서 사용 할 수 있도록 처리\n",
    "        df.to_parquet('./VL04_data.parquet', compression='gzip') #압축 파일 형식(gzip, snappy, ..)을 지정하여 pqrquet 파일 저장\n",
    "        \n",
    "    # tool 번호 및 상태 생성\n",
    "    if not df.empty and extract_tool_info:\n",
    "        extract_tool_info_from_data(df)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리 : 유효한 shot 정보 도출\n",
    " * 'shot_no' 필드 값을 기준으로 shot 구간 찾기\n",
    " * shot 구간 내부의 공구 사용 순서 및 각 공구 별 데이터 index 구간 도출\n",
    " * 공구 사용 순서에서 유효한 공구 사용 패턴 구간 찾기\n",
    " * 유효한 공구 사용 패턴 구간에 대해 numbering하여 별도의 'real_shot' 필드에 값 설정 (디폴트 : -1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataframe에 사용된 공구 순서 도출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-05T23:56:56.979825Z",
     "start_time": "2022-10-05T23:56:56.967827Z"
    }
   },
   "outputs": [],
   "source": [
    "#dataframe에 사용된 공구를 순서대로 추출해서 list형태로 반환\n",
    "def extract_tool_list(data, report_result=False):\n",
    "    import itertools\n",
    "\n",
    "    tool_list = []\n",
    "    #print([(k, (g)) for k, g in itertools.groupby(df_shot_specific['tool'])])\n",
    "    for k, g in itertools.groupby(data['tool']):\n",
    "        tool_list.append(k)\n",
    "\n",
    "    if report_result is True:\n",
    "        print(tool_list)\n",
    "    \n",
    "    return tool_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-05T23:56:56.994827Z",
     "start_time": "2022-10-05T23:56:56.981828Z"
    }
   },
   "outputs": [],
   "source": [
    "#dataframe에 사용된 공구를 순서대로 추출해서 list형태로 반환\n",
    "def extract_tool_list_with_range(data, report_result=False):\n",
    "    import itertools\n",
    "\n",
    "    irow = 0\n",
    "    tool_list = []\n",
    "    range_list_of_tool = []\n",
    "    \n",
    "    #print([(k, (g)) for k, g in itertools.groupby(df_shot_specific['tool'])])\n",
    "    for k, g in itertools.groupby(data['tool']):\n",
    "        tool_list.append(k)\n",
    "        nrow = len(list(g))\n",
    "        range_list_of_tool.append((irow,irow+nrow-1))\n",
    "        irow += nrow\n",
    "\n",
    "    if report_result is True:\n",
    "        print(tool_list)\n",
    "        print(range_list_of_tool)\n",
    "    \n",
    "    return tool_list, range_list_of_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-05T23:56:57.009829Z",
     "start_time": "2022-10-05T23:56:56.998828Z"
    }
   },
   "outputs": [],
   "source": [
    "#dataframe에서 (1) 주어진 shot_no에 해당하는 데이터를 추출한 후, (2) 해당 데이터에 사용된 공구를 순서대로 가져와서 반환\n",
    "def extract_tool_list_of_specific_shot(data, shot_no, report_result=False):\n",
    "    df_shot = data[data['shot_no'] == shot_no]\n",
    "    \n",
    "    if report_result is True:\n",
    "        print(\"===== shot{} ============================\\n\\n(1) dataframe -----------------\\n\".format(shot_no))\n",
    "        print(df_shot)\n",
    "        print(\"(2) used tool -----------------\\n\")\n",
    "        print(df_shot['tool'].unique())\n",
    "        print(\"(3) extract order using tools -----------------\\n\")\n",
    "    \n",
    "    tool_list = extract_tool_list(df_shot, report_result)\n",
    "    \n",
    "    del df_shot\n",
    "    \n",
    "    return tool_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-05T23:56:57.024828Z",
     "start_time": "2022-10-05T23:56:57.011829Z"
    }
   },
   "outputs": [],
   "source": [
    "#dataframe에서 전체 shot에 대해 아래의 과정 수행\n",
    "#   - 각 shot 별로 사용된 공구 순서를 추출\n",
    "#   - shot & 공구 사용 순서정보를 column으로 하는 dataframe을 구성한 후 반환\n",
    "def extract_tool_list_of_all_shots(data, report_result=False):\n",
    "    column_names = ['shot_no', 'order_using_tool']\n",
    "    df_by_shot = pd.DataFrame(columns=column_names)\n",
    "    row = 0\n",
    "\n",
    "    for shot in df['shot_no'].unique():\n",
    "        tool_list = extract_tool_list_of_specific_shot(df, shot, report_result)\n",
    "        str_tool_list = ' '.join(map(str, tool_list))\n",
    "        df_by_shot.loc[row] = [shot, str_tool_list]\n",
    "        \n",
    "        del tool_list\n",
    "        \n",
    "        row += 1\n",
    "\n",
    "    print(df_by_shot)\n",
    "    \n",
    "    return df_by_shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 유효 shot 구간 도출\n",
    " * 정상 공구 사용 패턴을 가지는 구간을 하나의 유효 shot으로 판별하고 별도의 column('real_shot')에 해당 numbering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-05T23:56:57.039828Z",
     "start_time": "2022-10-05T23:56:57.026828Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_sub_list(sl,l):\n",
    "    results=[]\n",
    "    sll=len(sl)\n",
    "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
    "        if l[ind:ind+sll]==sl:\n",
    "            results.append((ind,ind+sll-1))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-05T23:56:57.054826Z",
     "start_time": "2022-10-05T23:56:57.041828Z"
    }
   },
   "outputs": [],
   "source": [
    "#dataframe에서 (1) 주어진 shot_no에 해당하는 데이터를 추출한 후\n",
    "#(2) 해당 데이터에 사용된 공구 순서 및 각 공구 별 row index 범위를 순서대로 가져옴\n",
    "#(3) 정상 패턴 가공이 이루어지는 구간을 도출 (유효한 shot으로 판단할 수 있는 구간)\n",
    "#(4) (3)에서 찾은 구간 데이터의 'real_shot' 컬럼에 새로 shot numbering\n",
    "def find_and_mark_valid_shot_of_specific_shot(data, shot_no, real_shot_no, report_result=False):\n",
    "    df_shot = data[data['shot_no'] == shot_no]\n",
    "    \n",
    "    if report_result is True:\n",
    "        print(\"===== shot{} ============================\\n\\n(1) dataframe -----------------\\n\".format(shot_no))\n",
    "        print(df_shot)\n",
    "        print(\"(2) used tool -----------------\\n\")\n",
    "        print(df_shot['tool'].unique())\n",
    "        print(\"(3) extract order using tools -----------------\\n\")\n",
    "    \n",
    "    tool_list, range_list_of_tool = extract_tool_list_with_range(df_shot, report_result=False)\n",
    "\n",
    "    tool_pattern = [1, 5, 9, 11, 7, 3, 11, 7]\n",
    "    sub_list = find_sub_list(tool_pattern, tool_list)\n",
    "    \n",
    "    if len(sub_list) < 1:\n",
    "        if report_result is True:\n",
    "            print(\"shot_no {} of raw data : invalid_shot\".format(shot_no))\n",
    "\n",
    "        return 0\n",
    "    \n",
    "    if report_result is True:\n",
    "        print(sub_list)\n",
    "    \n",
    "    start_index = df_shot.index[0]\n",
    "    count = 0\n",
    "    \n",
    "    if report_result is True: \n",
    "        if sub_list[0][0] > 0:\n",
    "            print(\"------ invalid data\\n\")\n",
    "            print(data.iloc[start_index: start_index+range_list_of_tool[sub_list[0][0]][0]])\n",
    "\n",
    "    for sub in sub_list:\n",
    "        real_shot_no += 1\n",
    "        sub_start = start_index + range_list_of_tool[sub[0]][0]\n",
    "        sub_end   = start_index + range_list_of_tool[sub[1]][1]\n",
    "        print(\"sub({}:{}) - real_shot_no {}\".format(sub_start, sub_end, real_shot_no))\n",
    "        \n",
    "        #========================================================================\n",
    "        #LJY_20220926 : iloc 함수를 사용하여 구간 access 후 값 설정 시 pandas ver.1.3.5 이후로(1.4.0부터) 동작하지 않던 오류 수정\n",
    "        #------------------------------------------------------------\n",
    "        #오류 : 기존에 작성된 코드\n",
    "        #data.iloc[sub_start:sub_end]['real_shot'] = real_shot_no\n",
    "        \n",
    "        #------------------------------------------------------------\n",
    "        #방법1 : column을 특정한 후 row index range 지정하여 값 설정\n",
    "        data.real_shot.iloc[sub_start:sub_end] = real_shot_no\n",
    "        \n",
    "        #------------------------------------------------------------\n",
    "        #방법2 : row index range와 column index를 지정하여 값 설정\n",
    "        # print('real_shot_no : ',real_shot_no)\n",
    "        # data.iloc[sub_start:sub_end+1, data.columns.get_loc('real_shot')] = real_shot_no\n",
    "        #========================================================================\n",
    "        \n",
    "        if report_result is True:\n",
    "            print(\"------ valid shot[{}]\\n\".format(count))\n",
    "            print(data.iloc[sub_start:sub_end+1])\n",
    "            \n",
    "        count += 1\n",
    "    \n",
    "    tool_list.clear()\n",
    "    range_list_of_tool.clear()\n",
    "    sub_list.clear()\n",
    "    \n",
    "    del df_shot\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-05T23:56:57.069829Z",
     "start_time": "2022-10-05T23:56:57.056830Z"
    }
   },
   "outputs": [],
   "source": [
    "#dataframe에서 유효한 공구 사용 패턴을 가지는 구간을 찾아서 별도의 컬럼('real_shot')에 새로 number 부여\n",
    "# - 유효하지 않은 구간의 경우 'real_shot' 값을 -1로 설정\n",
    "def find_and_mark_valid_shot(data, report_result=False):\n",
    "    real_shot_no = 0\n",
    "    \n",
    "    data['real_shot'] = -1\n",
    "    \n",
    "    for shot in data['shot_no'].unique():\n",
    "        real_shot_no += find_and_mark_valid_shot_of_specific_shot(data, shot, real_shot_no, report_result)\n",
    "        print(real_shot_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 유효 shot 구간 내의 시작 부위의 유휴시간 데이터 삭제\n",
    " * 정상 공구 사용 패턴을 가지는 구간을 하나의 유효 shot으로 판별하고 별도의 column('real_shot')에 해당 numbering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-05T23:56:57.084831Z",
     "start_time": "2022-10-05T23:56:57.071829Z"
    }
   },
   "outputs": [],
   "source": [
    "#dataframe의 시작 부위의 idle section에 대한 'real_shot' 정보 초기화(값: -1)\n",
    "# => 실제 가공이 이루어진 유효한 구간을 찾아냄\n",
    "def remove_idle_section_at_the_start_of_valid_shot(data):\n",
    "    #print(data)\n",
    "    \n",
    "    #스핀들 전류값이 0보다 크면 가공이 이루어진 구간으로 판단하여 아무 처리하지 않고 return\n",
    "    if data.iloc[0]['current_spindle'] > 0:\n",
    "        return\n",
    "    \n",
    "    last_index_of_idle_section = data[data['current_spindle'] > 0].index[0]-1\n",
    "    \n",
    "    print(\"index : {}~{}\\n\".format(data.index[0], last_index_of_idle_section))\n",
    "    #print(data.loc[data.index[0]:last_index_of_idle_section])\n",
    "    data.loc[data.index[0]:last_index_of_idle_section, 'real_shot'] = -1 #idle 구간의 valid shot 정보 초기화\n",
    "    #print(data.loc[data.index[0]:last_index_of_idle_section])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-05T23:56:57.099828Z",
     "start_time": "2022-10-05T23:56:57.086830Z"
    }
   },
   "outputs": [],
   "source": [
    "#dataframe의 유효한 shot 구간('real_shot' != -1인 구간) 별로 시작 부위의 idle section에 대한 'real_shot' 정보 초기화(값: -1)\n",
    "# => 실제 가공이 이루어진 유효한 구간을 찾아냄\n",
    "def remove_idle_sections_at_the_start_of_valid_shots(data):\n",
    "    for shot in data['real_shot'].unique():\n",
    "        if shot < 0:\n",
    "            continue\n",
    "            \n",
    "#        df_by_shot = data[data['real_shot'] == shot]\n",
    "        \n",
    "#        if df_by_shot.iloc[0]['shot_no'] == 6517:\n",
    "#            print(df_by_shot)\n",
    "            \n",
    "#        print(df_by_shot)\n",
    "        index = data[data['real_shot'] == shot].index\n",
    "    \n",
    "        remove_idle_section_at_the_start_of_valid_shot(data.loc[index[0]:index[0]+len(index)-1])\n",
    "        \n",
    "        print(data.loc[index[0]:index[0]+len(index)-1])\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-05T23:56:57.114830Z",
     "start_time": "2022-10-05T23:56:57.101828Z"
    }
   },
   "outputs": [],
   "source": [
    "#dataframe에서 컬럼을 다중선택하여 데이터를 차트로 가시화\n",
    "#① data : dataframe 입력\n",
    "#② column_names : list 형태의 column명을 입력   ex) ['col1'], ['col1', 'col2'] \n",
    "#③ dataframe에서 특정 범위만을 선택해서 차트를 가시화 하려면 index_min 또는 index_max를 지정\n",
    "#   .. index_min : 디폴트값(-1)인 경우에는 데이터의 시작 row부터 포함됨\n",
    "#   ..  index_max : 디폴트값(-1)인 경우에는 데이터의 끝 row까지 포함됨\n",
    "#④ 차트 제목 표시 관련 설정\n",
    "#   .. title : 차트의 제목 문자열\n",
    "#   .. title_font_size : 차트의 제목을 표시할 font size\n",
    "#⑤ 차트 크기 설정\n",
    "#   .. figsize_horz, figsize_vert : 차트의 가로, 세로 크기\n",
    "#⑥ 범례 표시 관련 설정\n",
    "#   .. legend_font_size : 범례를 표시할 font size\n",
    "#   .. legend_location : 범례 표시 위치 (\"upper right\", \"lower right\", \"upper left\", \"lower left\")\n",
    "#⑦ 그래프 표현 관련 설정 : 선 or marker 표시\n",
    "#   .. linestyle : 선 스타일의 이름을 \"solid\", \"dashed\", \"dotted\", \"dashdot\"와 같은 형식으로 입력하거나 아래를 참고하여 입력\n",
    "'''\n",
    "'-'  solid line style\n",
    "'--' line style\n",
    "'-.' dash-dot line style\n",
    "':'  dotted line style\n",
    "'.'  point marker\n",
    "','  pixel marker\n",
    "'o'  circle marker\n",
    "'v'  triangle_down marker\n",
    "'^'  triangle_up marker\n",
    "'<'  triangle_left marker\n",
    "'>'  triangle_right marker\n",
    "'1'  tri_down marker\n",
    "'2'  tri_up marker\n",
    "'3'  tri_left marker\n",
    "'4'  tri_right marker\n",
    "'s'  square marker\n",
    "'p'  pentagon marker\n",
    "'*'  star marker\n",
    "'h'  hexagon1 marker\n",
    "'H'  hexagon2 marker\n",
    "'+'  plus marker\n",
    "'x'  x marker\n",
    "'D'  diamond marker\n",
    "'d'  thin_diamond marker\n",
    "'|'  vline marker\n",
    "'_'  hline marker\n",
    "'''\n",
    "#  .. marker : 아래를 참고하여 입력 ( https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html)\n",
    "'''\n",
    "'.'  point marker\n",
    "','  pixel marker\n",
    "'o'  circle marker\n",
    "'v'  triangle_down marker\n",
    "'^'  triangle_up marker\n",
    "'<'  triangle_left marker\n",
    "'>'  triangle_right marker\n",
    "'1'  tri_down marker\n",
    "'2'  tri_up marker\n",
    "'3'  tri_left marker\n",
    "'4'  tri_right marker\n",
    "'8'  octagon marker\n",
    "'s'  square marker\n",
    "'p'  pentagon marker\n",
    "'P'  plus (filled) marker\n",
    "'*'  star marker\n",
    "'h'  hexagon1 marker\n",
    "'H'  hexagon2 marker\n",
    "'+'  plus marker\n",
    "'x'  x marker\n",
    "'X'  x (filled) marker\n",
    "'D'  diamond marker\n",
    "'d'  thin_diamond marker\n",
    "'|'  vline marker\n",
    "'_'  hline marker\n",
    "'''\n",
    "def show_plot(data, column_names, index_min = -1, index_max = -1, title = None, title_font_size=20,\n",
    "              x_axis_title = None, y_axis_title = None, axes_title_font_size=18, tick_label_font_size=15,\n",
    "              figsize_horz=20, figsize_vert=10, legend_font_size=15, legend_location=\"upper right\",\n",
    "              linestyle = 'none', marker='.', marker_size=10):\n",
    "    \n",
    "    parameters = { 'figure.titlesize':title_font_size, 'axes.titlesize': axes_title_font_size,\n",
    "                  'axes.labelsize' : tick_label_font_size, 'legend.fontsize' : legend_font_size }\n",
    "    plt.rcParams.update(parameters)\n",
    "    \n",
    "    plt.figure(figsize = (figsize_horz, figsize_vert))\n",
    "    \n",
    "    str_col_names = '{}'.format(','.join(column_names))#(f\"'{col}'\" for col in column_name))\n",
    "    print(str_col_names)\n",
    "    \n",
    "    if index_min > -1 and index_max > -1 and index_min < index_max:\n",
    "        for column in list(column_names):\n",
    "            plt.plot(data.index[index_min:index_max], data.iloc[index_min:index_max][column], marker=marker, linestyle=linestyle, markersize=marker_size)\n",
    "    elif index_min > -1:\n",
    "        for column in list(column_names):\n",
    "            plt.plot(data.index[index_min:], data.iloc[index_min:][column], marker=marker, linestyle=linestyle, markersize=marker_size)\n",
    "    elif index_max > -1:\n",
    "        for column in list(column_names):\n",
    "            plt.plot(data.index[:index_max], data.iloc[:index_max][column], marker=marker, linestyle=linestyles, markersize=marker_size)\n",
    "    else:\n",
    "        #str_col_names = ','.join('{0}'.format(col) for col in column_name)\n",
    "        for column in list(column_names):\n",
    "            plt.plot(data.index, data[column], marker=marker, linestyle=linestyle, markersize=marker_size)\n",
    "    \n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "    plt.legend(column_names, loc = legend_location)\n",
    "    plt.xticks(fontsize = tick_label_font_size)\n",
    "    plt.yticks(fontsize = tick_label_font_size)\n",
    "    \n",
    "    if x_axis_title is not None:\n",
    "        plt.xlabel(x_axis_title)\n",
    "    if y_axis_title is not None:\n",
    "        plt.ylabel(y_axis_title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-05T23:56:57.129827Z",
     "start_time": "2022-10-05T23:56:57.117830Z"
    }
   },
   "outputs": [],
   "source": [
    "#dataframe에서 컬럼을 다중선택하여 데이터를 차트로 가시화\n",
    "#① data : dataframe 입력\n",
    "#② target_column_name : 값을 관찰할 컬럼\n",
    "#③ condition_column : 조건 판별 대상 컬럼명  ex) 'state'\n",
    "#④ condition_values : 일치 여부 대상 value 목록  ex) [1], [0, 1]\n",
    "#⑤ dataframe에서 특정 범위만을 선택해서 차트를 가시화 하려면 index_min 또는 index_max를 지정\n",
    "#   .. index_min : 디폴트값(-1)인 경우에는 데이터의 시작 row부터 포함됨\n",
    "#   ..  index_max : 디폴트값(-1)인 경우에는 데이터의 끝 row까지 포함됨\n",
    "#⑥ 차트 제목 표시 관련 설정\n",
    "#   .. title : 차트의 제목 문자열\n",
    "#   .. title_font_size : 차트의 제목을 표시할 font size\n",
    "#⑦ 차트 크기 설정\n",
    "#   .. figsize_horz, figsize_vert : 차트의 가로, 세로 크기\n",
    "#⑧ 범례 표시 관련 설정\n",
    "#   .. legend_font_size : 범례를 표시할 font size\n",
    "#   .. legend_location : 범례 표시 위치 (\"upper right\", \"lower right\", \"upper left\", \"lower left\")\n",
    "#⑨ 그래프 표현 관련 설정 : 선 or marker 표시\n",
    "#   .. linestyle : 선 스타일의 이름을 \"solid\", \"dashed\", \"dotted\", \"dashdot\"와 같은 형식으로 입력하거나 아래를 참고하여 입력\n",
    "'''\n",
    "'-'  solid line style\n",
    "'--' line style\n",
    "'-.' dash-dot line style\n",
    "':'  dotted line style\n",
    "'.'  point marker\n",
    "','  pixel marker\n",
    "'o'  circle marker\n",
    "'v'  triangle_down marker\n",
    "'^'  triangle_up marker\n",
    "'<'  triangle_left marker\n",
    "'>'  triangle_right marker\n",
    "'1'  tri_down marker\n",
    "'2'  tri_up marker\n",
    "'3'  tri_left marker\n",
    "'4'  tri_right marker\n",
    "'s'  square marker\n",
    "'p'  pentagon marker\n",
    "'*'  star marker\n",
    "'h'  hexagon1 marker\n",
    "'H'  hexagon2 marker\n",
    "'+'  plus marker\n",
    "'x'  x marker\n",
    "'D'  diamond marker\n",
    "'d'  thin_diamond marker\n",
    "'|'  vline marker\n",
    "'_'  hline marker\n",
    "'''\n",
    "#  .. marker : 아래를 참고하여 입력 ( https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html)\n",
    "'''\n",
    "'.'  point marker\n",
    "','  pixel marker\n",
    "'o'  circle marker\n",
    "'v'  triangle_down marker\n",
    "'^'  triangle_up marker\n",
    "'<'  triangle_left marker\n",
    "'>'  triangle_right marker\n",
    "'1'  tri_down marker\n",
    "'2'  tri_up marker\n",
    "'3'  tri_left marker\n",
    "'4'  tri_right marker\n",
    "'8'  octagon marker\n",
    "'s'  square marker\n",
    "'p'  pentagon marker\n",
    "'P'  plus (filled) marker\n",
    "'*'  star marker\n",
    "'h'  hexagon1 marker\n",
    "'H'  hexagon2 marker\n",
    "'+'  plus marker\n",
    "'x'  x marker\n",
    "'X'  x (filled) marker\n",
    "'D'  diamond marker\n",
    "'d'  thin_diamond marker\n",
    "'|'  vline marker\n",
    "'_'  hline marker\n",
    "'''\n",
    "def show_plot_comparing_data_by_condition(data, target_column_name, condition_column, condition_values, data_names = ('condition is true','condition is false'),\n",
    "                                           index_min = -1, index_max = -1, title = None, title_font_size=20,\n",
    "                                           x_axis_title = None, y_axis_title = None, axes_title_font_size=18,\n",
    "                                           tick_label_font_size=15, figsize_horz=20, figsize_vert=10,\n",
    "                                           legend_font_size=15, legend_location=\"lower right\", linestyle = 'none',\n",
    "                                           marker='.', marker_size=10):\n",
    "    \n",
    "    parameters = { 'figure.titlesize':title_font_size, 'axes.titlesize': axes_title_font_size,\n",
    "                  'axes.labelsize' : tick_label_font_size, 'legend.fontsize' : legend_font_size }\n",
    "    plt.rcParams.update(parameters)\n",
    "    \n",
    "    plt.figure(figsize = (figsize_horz, figsize_vert))\n",
    "    \n",
    "    condition_true_data = pd.DataFrame()\n",
    "    condition_false_data = pd.DataFrame()\n",
    "      \n",
    "    if index_min > -1 and index_max > -1 and index_min < index_max:\n",
    "        condition_true_data = data.iloc[index_min:index_max].query(data[condition_column].isin(condition_values))\n",
    "        condition_false_data = data.iloc[index_min:index_max].query(~data[condition_column].isin(condition_values))\n",
    "    elif index_min > -1:\n",
    "        condition_true_data = data.iloc[index_min:].query(data[condition_column].isin(condition_values))\n",
    "        condition_false_data = data.iloc[index_min:].query(~data[condition_column].isin(condition_values))        \n",
    "    elif index_max > -1:\n",
    "        condition_true_data = data.iloc[:index_max].query(data[condition_column].isin(condition_values))\n",
    "        condition_false_data = data.iloc[:index_max].query(~data[condition_column].isin(condition_values))        \n",
    "    else:\n",
    "        condition_true_data = data[data[condition_column].isin(condition_values)]\n",
    "        condition_false_data = data[~data[condition_column].isin(condition_values)]\n",
    "        \n",
    "    plt.plot(condition_true_data.index, condition_true_data[target_column_name], marker=marker, linestyle=linestyle, label=data_names[0], color='red', markersize=marker_size)\n",
    "    plt.plot(condition_false_data.index, condition_false_data[target_column_name], marker=marker, linestyle=linestyle, label=data_names[1], markersize=marker_size)\n",
    "    \n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "    plt.legend(data_names, loc = legend_location)\n",
    "    plt.xticks(fontsize = tick_label_font_size)\n",
    "    plt.yticks(fontsize = tick_label_font_size)\n",
    "    \n",
    "    if x_axis_title is not None:\n",
    "        plt.xlabel(x_axis_title, fontsize = axes_title_font_size)\n",
    "    if y_axis_title is not None:\n",
    "        plt.ylabel(y_axis_title, fontsize = axes_title_font_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 수립"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 유효한 shot을 기준으로 주어진 공구에 대한 주어진 field의 대푯값을 계산하여 dataframe 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_outliers(data, column):\n",
    "    Q1 = np.percentile(data[column], 25)\n",
    "    Q3 = np.percentile(data[column], 75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    outlier_step = 1.5 * IQR\n",
    "    \n",
    "    print('Q1 = {}, Q3 = {}, IQR = {}, outlier_step = {}, range = ({}. {})'.format(Q1, Q3, IQR, outlier_step, Q1-outlier_step, Q3+outlier_step))\n",
    "\n",
    "    outlier_index = data[(data[column] < Q1) | (data[column] > Q3 + outlier_step)].index\n",
    "        \n",
    "    print(outlier_index)\n",
    "    \n",
    "    data.drop(outlier_index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-05T23:56:57.174828Z",
     "start_time": "2022-10-05T23:56:57.146830Z"
    }
   },
   "outputs": [],
   "source": [
    "# 유효한 shot을 기준으로 주어진 공구에 대한 주어진 field의 대푯값을 계산하여  dataframe으로 구성\n",
    "def build_representative_data_for_tool_machining(data, tool_no, target_column_name, apply_robust_scaler=False, except_min_value=False, except_max_value=False, delete_outlier=False,\n",
    "                                                 delete_zero_value=False, except_tool_cancel_state=False, except_tool_end_state=False):\n",
    "    if target_column_name not in data.columns:\n",
    "        print(\"{} is not in columns\\nPlease input valid column name\".format(target_column_name))\n",
    "        return\n",
    "    \n",
    "    condition = (data['tool']==tool_no) & (data['real_shot'] != -1)\n",
    "    \n",
    "    #LJY_20221005 : tool취소 상태 데이터 제거 옵션 추가\n",
    "    if except_tool_cancel_state is True:\n",
    "        condition = condition & (data['tool_state'] != 0)\n",
    "        \n",
    "    if except_tool_end_state is True:\n",
    "        condition = condition & (data['tool_state'] != 9)\n",
    "        \n",
    "    #data_tool = data[(data['tool']==tool_no) & (data['real_shot'] != -1)]\n",
    "    data_tool = data[condition]\n",
    "    \n",
    "    if delete_zero_value is True:\n",
    "        data_tool.drop(data_tool[data_tool[target_column_name] == 0].index, inplace=True)\n",
    "        \n",
    "    if delete_outlier is True:\n",
    "        print('before deleting outliers : {}'.format(len(data_tool.index)))\n",
    "        delete_outliers(data_tool, target_column_name)\n",
    "        print('after deleting outliers : {}'.format(len(data_tool.index)))\n",
    "    \n",
    "    if except_min_value is True:\n",
    "        min_value = data_tool[target_column_name].min()\n",
    "        data_tool.drop(data_tool[data_tool[target_column_name] == min_value].index, inplace=True)\n",
    "        \n",
    "    if except_max_value is True:\n",
    "        max_value = data_tool[target_column_name].max()\n",
    "        data_tool.drop(data_tool[data_tool[target_column_name] == max_value].index, inplace=True)\n",
    "    \n",
    "    if apply_robust_scaler is True:\n",
    "        from sklearn import metrics\n",
    "        #from sklearn.preprocessing import MinMaxScaler\n",
    "        #from sklearn.preprocessing import StandardScaler\n",
    "        from sklearn.preprocessing import RobustScaler\n",
    "        \n",
    "        rs = RobustScaler()\n",
    "        #ms = MinMaxScaler()\n",
    "        data_tool[[target_column_name]] = rs.fit_transform(data_tool[[target_column_name]])\n",
    "    \n",
    "    #real shot no.\n",
    "#    data_shot_no = data_tool['real_shot'].to_frame(name='real_shot')\n",
    "    group_by_shot = data_tool.groupby(['real_shot'])\n",
    "    #print(group_by_shot.groups.keys())\n",
    "    \n",
    "    #표준편차\n",
    "    data_std = (group_by_shot.std()[target_column_name].to_frame(name='std'))\n",
    "    #표준오차\n",
    "    data_sem = (group_by_shot.sem()[target_column_name].to_frame(name='sem'))\n",
    "    #합\n",
    "    data_sum = (group_by_shot.sum()[target_column_name].to_frame(name='sum'))\n",
    "    #평균\n",
    "    data_mean = (group_by_shot.mean()[target_column_name].to_frame(name='mean'))\n",
    "    #중앙값\n",
    "    data_median = (group_by_shot.median()[target_column_name].to_frame(name='median'))\n",
    "    #분산\n",
    "    data_var = (group_by_shot.var()[target_column_name].to_frame(name='var'))\n",
    "    #최대값\n",
    "    data_max = (group_by_shot.max()[target_column_name].to_frame(name='max')) \n",
    "    #0.25분위수\n",
    "    data_quantile_1_per_4 = (group_by_shot.quantile(0.25)[target_column_name].to_frame(name='quantile(0.25)')) \n",
    "    #0.75분위수\n",
    "    data_quantile_3_per_4 = (group_by_shot.quantile(0.75)[target_column_name].to_frame(name='quantile(0.75)')) \n",
    "    #왜도\n",
    "    data_skew = (group_by_shot.skew()[target_column_name].to_frame(name='skew')) \n",
    "\n",
    "    data_representative = pd.concat([data_mean, data_std, data_median, data_max, data_var, data_sem, data_sum, data_quantile_1_per_4, data_quantile_3_per_4, data_skew],axis=1)\n",
    "    \n",
    "    data_representative.set_index(pd.Series(group_by_shot.groups.keys()))\n",
    "    \n",
    "    return data_representative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다변량 처리를 위한 함수 정의\n",
    "# 유효한 shot을 기준으로 주어진 공구에 대한 주어진 field의 대푯값을 계산하여  dataframe으로 구성\n",
    "# 유효한 shot을 기준으로 주어진 공구에 대한 주어진 field의 대푯값을 계산하여  dataframe으로 구성\n",
    "def build_representative_multi_data_for_tool_machining(data, tool_no, stat='mean', apply_robust_scaler=False, except_min_value=False, except_max_value=False, delete_outlier=False,\n",
    "                                                 delete_zero_value=False, except_tool_cancel_state=False, except_tool_end_state=False, *target_column_name):\n",
    "    # stat = std, sem, sum, mean, median, var, max, 25%, 75%, skew, overall\n",
    "    target_column_name = list(target_column_name)\n",
    "    for column_idx in target_column_name:\n",
    "        if column_idx not in data.columns:\n",
    "            print(\"{} is not in columns\\n Please input valid column name\".format(column_idx))\n",
    "            return\n",
    "    condition = (data['tool']==tool_no) & (data['real_shot'] != -1)\n",
    "    \n",
    "    #LJY_20221005 : tool취소 상태 데이터 제거 옵션 추가\n",
    "    if except_tool_cancel_state is True:\n",
    "        condition = condition & (data['tool_state'] != 0)\n",
    "        \n",
    "    if except_tool_end_state is True:\n",
    "        condition = condition & (data['tool_state'] != 9)\n",
    "        \n",
    "    #data_tool = data[(data['tool']==tool_no) & (data['real_shot'] != -1)]\n",
    "    data_tool = data[condition]\n",
    "    \n",
    "    if delete_zero_value is True:\n",
    "        zero_index = []\n",
    "        for column_idx2 in target_column_name:\n",
    "            zero_index.append(data_tool[data_tool[column_idx2] == 0].index)\n",
    "        zero_result = list(set(zero_index))\n",
    "        print(zero_result)\n",
    "        data_tool.drop(data_tool[data_tool[column_idx2] == 0].index, inplace=True)\n",
    "        \n",
    "    if delete_outlier is True:\n",
    "        print('before deleting outliers : {}'.format(len(data_tool.index)))\n",
    "        for column_idx3 in target_column_name:\n",
    "            delete_outliers(data_tool, column_idx3)\n",
    "        print('after deleting outliers : {}'.format(len(data_tool.index)))\n",
    "    \n",
    "    if except_min_value is True:\n",
    "        for column_idx4 in target_column_name:\n",
    "            globals()[\"{}_min_value\".format(column_idx4)] = data_tool[column_idx4].min()\n",
    "            data_tool.drop(data_tool[data_tool[column_idx4] == globals[\"{}_min_value\".format(column_idx4)]].index, inplace=True)\n",
    "        \n",
    "    if except_max_value is True:\n",
    "        for column_idx5 in target_column_name:\n",
    "            globals()[\"{}_max_value\".format(column_idx5)] = data_tool[column_idx5].max()\n",
    "            data_tool.drop(data_tool[data_tool[column_idx5] == globals()[\"{}_max_value\".format(column_idx5)]].index, inplace=True)\n",
    "    \n",
    "    if apply_robust_scaler is True:\n",
    "        from sklearn import metrics\n",
    "        #from sklearn.preprocessing import MinMaxScaler\n",
    "        #from sklearn.preprocessing import StandardScaler\n",
    "        from sklearn.preprocessing import RobustScaler\n",
    "        \n",
    "        rs = RobustScaler()\n",
    "        #ms = MinMaxScaler()\n",
    "        data_info = data_tool[['datetime', 'shot_no', 'tool_number', 'tool_state', 'tool', 'real_shot']]\n",
    "        data_tool = data_tool.drop(data_info.columns, axis = 1)\n",
    "        data_tool[target_column_name] = rs.fit_transform(data_tool[target_column_name])\n",
    "        data_tool = pd.concat([data_info, data_tool], axis = 1)\n",
    "    \n",
    "    \n",
    "    #real shot no.\n",
    "#    data_shot_no = data_tool['real_shot'].to_frame(name='real_shot')\n",
    "    group_by_shot = data_tool.groupby(['real_shot'])\n",
    "#    print(group_by_shot.groups.keys())\n",
    "    \n",
    "    #표준편차\n",
    "    # data_std = pd.DataFrame(group_by_shot.std()[target_column_name], columns = ['std_' + str(i) for i in target_column_name])\n",
    "    \n",
    "    stat_list = ['std', 'sem', 'sum', 'mean', 'median', 'var', 'max', '25%', '75%', 'skew', 'overall']\n",
    "#     try : \n",
    "#         stat in stat_list == True\n",
    "#     except ValueError as val:\n",
    "#         print('대표값 목록에 있는 값으로 입력해주세요!')\n",
    "#         print('대표값 목록은 {}입니다.'.format(','.join()))\n",
    "#     else:\n",
    "    if stat == 'std':\n",
    "        data_representative = pd.DataFrame(group_by_shot.std()[target_column_name])\n",
    "        data_representative.columns = ['std_' + str(i) for i in target_column_name]\n",
    "    elif stat == 'sem':\n",
    "        data_representative = pd.DataFrame(group_by_shot.sem()[target_column_name])\n",
    "        data_representative.columns = ['sem_' + str(i) for i in target_column_name]\n",
    "    elif stat == 'sum':\n",
    "        data_representative = pd.DataFrame(group_by_shot.sum()[target_column_name])\n",
    "        data_representative.columns = ['sum_' + str(i) for i in target_column_name]\n",
    "    elif stat == 'mean':\n",
    "        data_representative = pd.DataFrame(group_by_shot.sum()[target_column_name])\n",
    "        data_representative.columns = ['mean_' + str(i) for i in target_column_name]\n",
    "    elif stat == 'median':\n",
    "        data_representative = pd.DataFrame(group_by_shot.sum()[target_column_name])\n",
    "        data_representative.columns = ['median_' + str(i) for i in target_column_name]\n",
    "    elif stat == 'var':\n",
    "        data_representative = pd.DataFrame(group_by_shot.sum()[target_column_name])\n",
    "        data_representative.columns = ['var_' + str(i) for i in target_column_name]\n",
    "    elif stat == 'max':\n",
    "        data_representative = pd.DataFrame(group_by_shot.sum()[target_column_name])\n",
    "        data_representative.columns = ['max_' + str(i) for i in target_column_name]\n",
    "    elif stat == '25%':\n",
    "        data_representative = pd.DataFrame(group_by_shot.sum()[target_column_name])\n",
    "        data_representative.columns = ['q1(25%)_' + str(i) for i in target_column_name]\n",
    "    elif stat == '75%':\n",
    "        data_representative = pd.DataFrame(group_by_shot.sum()[target_column_name])\n",
    "        data_representative.columns = ['q3(75%)_' + str(i) for i in target_column_name]\n",
    "    elif stat == 'skew':\n",
    "        data_representative = pd.DataFrame(group_by_shot.sum()[target_column_name])\n",
    "        data_representative.columns = ['skew_' + str(i) for i in target_column_name]\n",
    "    elif stat == 'overall' : \n",
    "        data_std = pd.DataFrame(group_by_shot.std()[target_column_name])\n",
    "        data_std.columns = ['std_' + str(i) for i in target_column_name]\n",
    "        #표준오차\n",
    "        data_sem = pd.DataFrame(group_by_shot.sem()[target_column_name])\n",
    "        data_sem.columns = ['sem_' + str(i) for i in target_column_name]\n",
    "        #합\n",
    "        data_sum = pd.DataFrame(group_by_shot.sum()[target_column_name])\n",
    "        data_sum.columns = ['sum_' + str(i) for i in target_column_name]\n",
    "        #평균\n",
    "        data_mean = pd.DataFrame(group_by_shot.mean()[target_column_name])\n",
    "        data_mean.columns = ['mean_' + str(i) for i in target_column_name]\n",
    "        #중앙값\n",
    "        data_median = pd.DataFrame(group_by_shot.median()[target_column_name])\n",
    "        data_median.columns = ['median_' + str(i) for i in target_column_name]\n",
    "        #분산\n",
    "        data_var = pd.DataFrame(group_by_shot.var()[target_column_name])\n",
    "        data_var.columns = ['var_' + str(i) for i in target_column_name]\n",
    "        #최대값\n",
    "        data_max = pd.DataFrame(group_by_shot.max()[target_column_name])\n",
    "        data_max.columns = ['max_' + str(i) for i in target_column_name]\n",
    "        #0.25분위수\n",
    "        data_quantile_1_per_4 = pd.DataFrame(group_by_shot.quantile(0.25)[target_column_name])\n",
    "        data_quantile_1_per_4.columns = ['q1_' + str(i) for i in target_column_name]\n",
    "        #0.75분위수\n",
    "        data_quantile_3_per_4 = pd.DataFrame(group_by_shot.quantile(0.75)[target_column_name])\n",
    "        data_quantile_3_per_4.columns = ['q3_' + str(i) for i in target_column_name]\n",
    "        #왜도\n",
    "        data_skew = pd.DataFrame(group_by_shot.skew()[target_column_name]) \n",
    "        data_skew.columns = ['skew_' + str(i) for i in target_column_name]\n",
    "\n",
    "        data_representative = pd.concat([data_mean, data_std, data_median, data_max, data_var, data_sem, data_sum, data_quantile_1_per_4, data_quantile_3_per_4, data_skew],axis=1)\n",
    "\n",
    "        data_representative.set_index(pd.Series(group_by_shot.groups.keys()))\n",
    "\n",
    "    return data_representative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주어진 공구의 주어진 필드에 대한 valid shot 별 대푯값 도출 & 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-05T23:56:57.189831Z",
     "start_time": "2022-10-05T23:56:57.177830Z"
    }
   },
   "outputs": [],
   "source": [
    "#representative_value_types : 'std', 'sem', 'sum', 'mean', 'median', 'var', 'max', 'quantile(0.25)', 'quantile(0.75)', 'skew'를 리스트 형태로 입력\n",
    "#  ex) ['std', 'mean']\n",
    "def build_and_display_representative_data_for_tool_machining(data, tool_no, target_column_name, representative_value_types, apply_robust_scaler=False, except_min_value=False, except_max_value=False):\n",
    "    df_representative = build_representative_data_for_tool_machining(data, tool_no, target_column_name, apply_robust_scaler, except_min_value, except_max_value)\n",
    "    show_plot(df_representative, representative_value_types, title = target_column_name, x_axis_title='real shot no.', y_axis_title='representatives')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주어진 필드에 대해 Isolation Forest 기법을 적용하여 이상치 탐지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-05T23:56:57.204825Z",
     "start_time": "2022-10-05T23:56:57.191828Z"
    }
   },
   "outputs": [],
   "source": [
    "# he offsetis defined in such a way we obtain the expected number of outliers (samples with decision function < 0) in training.\n",
    "def check_and_mark_outlier_by_IsolationForest_org(training_data, test_data, training_or_test = True, *target_column_name):\n",
    "    import pickle\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    import joblib\n",
    "    \n",
    "    \n",
    "    # 학습 모델    \n",
    "    target_column_name = list(target_column_name)\n",
    "    print(target_column_name)\n",
    "    if training_or_test == True:\n",
    "        data = training_data[target_column_name]\n",
    "        IF = IsolationForest(random_state = 42, contamination = 0.004 , n_estimators = 500, max_samples = 90, n_jobs = -1, bootstrap = True).fit(training_data[target_column_name])\n",
    "        score = IF.decision_function(data)\n",
    "        training_data['IF_Outliers'] = pd.Series(IF.predict(data), index = training_data.index).apply(lambda x: 1 if x == -1 else 0)\n",
    "        training_data['IF_score'] = score\n",
    "        joblib.dump(IF, os.path.join(os.getcwd(), 'IF_training_model.pkl'))\n",
    "        return training_data\n",
    "        # training_data['score_sample'] = IF.score_samples(training_data[[target_column_name]])\n",
    "        # training_data['offset'] = training_data['IF_score'] - training_data['score_sample']\n",
    "#         training_data.loc[(training_data['IF_Outliers'] == 1) & (training_data['IF_score'] < 0)]['IF_Outliers'] = 0\n",
    "#         training_data.loc[(training_data['IF_Outliers'] == 0) & (training_data['IF_score'] > 0)]['IF_Outliers'] = 1 \n",
    "    else:\n",
    "        try:\n",
    "            joblib.load(os.path.join(os.getcwd(), 'IF_training_model.pkl'))\n",
    "        except FileNotFoundError:\n",
    "            print('IF_training_model File does not exist.')\n",
    "            print('This need a learning model.')\n",
    "            print('Please training model establishment first.')\n",
    "        else:\n",
    "            training_model = joblib.load(os.path.join(os.getcwd(), 'IF_training_model.pkl'))\n",
    "            target_column_name = list(target_column_name)\n",
    "            data = test_data[target_column_name]\n",
    "            score = training_model.score_samples(data)\n",
    "            test_data['IF_Outliers'] = pd.Series(training_model.predict(data), index = test_data.index).apply(lambda x: 1 if x == -1 else 0)\n",
    "            test_data['IF_score'] = score\n",
    "            return test_data\n",
    "            # test_data['score_sample'] = IF.score_samples(test_data[[target_column_name]])\n",
    "            # test_data['offset'] = test_data['IF_score'] - test_data['score_sample'] # offset = decision_function - sample_score\n",
    "    #         test_data.loc[(test_data['IF_Outliers'] == 1) & (test_data['IF_score'] > -0.006), ['IF_Outliers']] = 0 \n",
    "    #         test_data.loc[(test_data['IF_Outliers'] == 0) & (test_data['IF_score'] < -0), ['IF_Outliers']] = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다변량 Isolation forest 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# he offsetis defined in such a way we obtain the expected number of outliers (samples with decision function < 0) in training.\n",
    "# 다변량 Isolation forest 처리\n",
    "def multi_check_and_mark_outlier_by_IsolationForest_org(training_data, test_data, training_or_test = True, *target_column_name):\n",
    "    import pickle\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    import joblib\n",
    "    \n",
    "    \n",
    "    # 학습 모델    \n",
    "    target_column_name = list(target_column_name)\n",
    "    for column_idx in target_column_name:\n",
    "        if column_idx not in training_data.columns:\n",
    "            print(\"{} is not in columns\\n Please input valid column name\".format(column_idx))\n",
    "            return\n",
    "        # 데이터프레임에 결측값이 존재하는 여부를 확인처리 필요 target_column_name에 대하여\n",
    "    try: \n",
    "        if training_data.isnull().sum().sum() > 0:\n",
    "            raise ValueError\n",
    "    except ValueError:\n",
    "        print('Training data include NaN value.')\n",
    "        print('First, It is essential that processing NaN value of training data.\\n')\n",
    "    try: \n",
    "        if test_data.isnull().sum().sum() > 0:\n",
    "            raise ValueError\n",
    "    except ValueError:\n",
    "        print('Test data include NaN value.')\n",
    "        print('First, It is essential that processing NaN value of test data.\\n')\n",
    "\n",
    "    else:\n",
    "        if training_or_test == True:\n",
    "            data = training_data[target_column_name]\n",
    "            IF = IsolationForest(random_state = 42, contamination = 0.004 , n_estimators = 500, max_samples = 90, n_jobs = -1, bootstrap = True).fit(data)\n",
    "            score = IF.decision_function(data)\n",
    "            training_data['IF_Outliers'] = pd.Series(IF.predict(data), index = training_data.index).apply(lambda x: 1 if x == -1 else 0)\n",
    "            training_data['IF_score'] = score\n",
    "            joblib.dump(IF, os.path.join(os.getcwd(), 'IF_training_model.pkl'))\n",
    "            return training_data\n",
    "            # training_data['score_sample'] = IF.score_samples(training_data[[target_column_name]])\n",
    "            # training_data['offset'] = training_data['IF_score'] - training_data['score_sample']\n",
    "    #         training_data.loc[(training_data['IF_Outliers'] == 1) & (training_data['IF_score'] < 0)]['IF_Outliers'] = 0\n",
    "    #         training_data.loc[(training_data['IF_Outliers'] == 0) & (training_data['IF_score'] > 0)]['IF_Outliers'] = 1 \n",
    "        else:\n",
    "            try:\n",
    "                joblib.load(os.path.join(os.getcwd(), 'IF_training_model.pkl'))\n",
    "            except FileNotFoundError:\n",
    "                print('IF_training_model File does not exist.')\n",
    "                print('This need a learning model.')\n",
    "                print('Please training model establishment first.')\n",
    "            else:\n",
    "                training_model = joblib.load(os.path.join(os.getcwd(), 'IF_training_model.pkl'))\n",
    "                target_column_name = list(target_column_name)\n",
    "                data = test_data[target_column_name]\n",
    "                score = training_model.score_samples(data)\n",
    "                test_data['IF_Outliers'] = pd.Series(training_model.predict(data), index = test_data.index).apply(lambda x: 1 if x == -1 else 0)\n",
    "                test_data['IF_score'] = score\n",
    "                return test_data\n",
    "                # test_data['score_sample'] = IF.score_samples(test_data[[target_column_name]])\n",
    "                # test_data['offset'] = test_data['IF_score'] - test_data['score_sample'] # offset = decision_function - sample_score\n",
    "        #         test_data.loc[(test_data['IF_Outliers'] == 1) & (test_data['IF_score'] > -0.006), ['IF_Outliers']] = 0 \n",
    "        #         test_data.loc[(test_data['IF_Outliers'] == 0) & (test_data['IF_score'] < -0), ['IF_Outliers']] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T00:34:42.856500Z",
     "start_time": "2022-10-06T00:34:42.838523Z"
    }
   },
   "outputs": [],
   "source": [
    "# def check_and_mark_outlier_by_IsolationForest_test(test_data, target_column_name):\n",
    "#     # applying test set    \n",
    "#     score = check_and_mark_outlier_by_IsolationForest_org.decision_function(test_data[[target_column_name]])\n",
    "#     data['IF_Outliers'] = pd.Series(check_and_mark_outlier_by_IsolationForest_org.predict(test_data[[target_column_name]])).apply(lambda x: 1 if x == -1 else 0)\n",
    "#     data['IF_score'] = score\n",
    "    \n",
    "#     # data.loc[(data['IF_Outliers'] == 1) & (data['IF_score'] > 0), ['IF_Outliers']] = 0 #LJY_20221005 : 'IF_score' 값이 0보다 큰 경우는 '이상'으로 판별하지 않도록 처리 시도\n",
    "#     # data.loc[(data['IF_Outliers'] == 0) & (data['IF_score'] < 0), ['IF_Outliers']] = 1 #LJY_20221006 : 'IF_score' 값이 0보다 작은 경우는 '정상'으로 판별하지 않도록 처리 시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe에서 컬럼을 다중선택하여 데이터를 차트로 가시화\n",
    "#① data : dataframe 입력\n",
    "#② target_column_name : 값을 관찰할 컬럼\n",
    "#③ condition_column : 조건 판별 대상 컬럼명  ex) 'state'\n",
    "#④ condition_values : 일치 여부 대상 value 목록  ex) [1], [0, 1]\n",
    "#⑤ dataframe에서 특정 범위만을 선택해서 차트를 가시화 하려면 index_min 또는 index_max를 지정\n",
    "#   .. index_min : 디폴트값(-1)인 경우에는 데이터의 시작 row부터 포함됨\n",
    "#   ..  index_max : 디폴트값(-1)인 경우에는 데이터의 끝 row까지 포함됨\n",
    "#⑥ 차트 제목 표시 관련 설정\n",
    "#   .. title : 차트의 제목 문자열\n",
    "#   .. title_font_size : 차트의 제목을 표시할 font size\n",
    "#⑦ 차트 크기 설정\n",
    "#   .. figsize_horz, figsize_vert : 차트의 가로, 세로 크기\n",
    "#⑧ 범례 표시 관련 설정\n",
    "#   .. legend_font_size : 범례를 표시할 font size\n",
    "#   .. legend_location : 범례 표시 위치 (\"upper right\", \"lower right\", \"upper left\", \"lower left\")\n",
    "#⑨ 그래프 표현 관련 설정 : 선 or marker 표시\n",
    "#   .. linestyle : 선 스타일의 이름을 \"solid\", \"dashed\", \"dotted\", \"dashdot\"와 같은 형식으로 입력하거나 아래를 참고하여 입력\n",
    "'''\n",
    "'-'  solid line style\n",
    "'--' line style\n",
    "'-.' dash-dot line style\n",
    "':'  dotted line style\n",
    "'.'  point marker\n",
    "','  pixel marker\n",
    "'o'  circle marker\n",
    "'v'  triangle_down marker\n",
    "'^'  triangle_up marker\n",
    "'<'  triangle_left marker\n",
    "'>'  triangle_right marker\n",
    "'1'  tri_down marker\n",
    "'2'  tri_up marker\n",
    "'3'  tri_left marker\n",
    "'4'  tri_right marker\n",
    "'s'  square marker\n",
    "'p'  pentagon marker\n",
    "'*'  star marker\n",
    "'h'  hexagon1 marker\n",
    "'H'  hexagon2 marker\n",
    "'+'  plus marker\n",
    "'x'  x marker\n",
    "'D'  diamond marker\n",
    "'d'  thin_diamond marker\n",
    "'|'  vline marker\n",
    "'_'  hline marker\n",
    "'''\n",
    "#  .. marker : 아래를 참고하여 입력 ( https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html)\n",
    "'''\n",
    "'.'  point marker\n",
    "','  pixel marker\n",
    "'o'  circle marker\n",
    "'v'  triangle_down marker\n",
    "'^'  triangle_up marker\n",
    "'<'  triangle_left marker\n",
    "'>'  triangle_right marker\n",
    "'1'  tri_down marker\n",
    "'2'  tri_up marker\n",
    "'3'  tri_left marker\n",
    "'4'  tri_right marker\n",
    "'8'  octagon marker\n",
    "'s'  square marker\n",
    "'p'  pentagon marker\n",
    "'P'  plus (filled) marker\n",
    "'*'  star marker\n",
    "'h'  hexagon1 marker\n",
    "'H'  hexagon2 marker\n",
    "'+'  plus marker\n",
    "'x'  x marker\n",
    "'X'  x (filled) marker\n",
    "'D'  diamond marker\n",
    "'d'  thin_diamond marker\n",
    "'|'  vline marker\n",
    "'_'  hline marker\n",
    "'''\n",
    "def show_plot_comparing_data_by_condition(data, target_column_name, condition_column, condition_values, data_names = ('condition is true','condition is false'),\n",
    "                                           index_min = -1, index_max = -1, title = None, title_font_size=20,\n",
    "                                           x_axis_title = None, y_axis_title = None, axes_title_font_size=18,\n",
    "                                           tick_label_font_size=15, figsize_horz=20, figsize_vert=10,\n",
    "                                           legend_font_size=15, legend_location=\"lower right\", linestyle = 'none',\n",
    "                                           marker='.', marker_size=10):\n",
    "    \n",
    "    parameters = { 'figure.titlesize':title_font_size, 'axes.titlesize': axes_title_font_size,\n",
    "                  'axes.labelsize' : tick_label_font_size, 'legend.fontsize' : legend_font_size }\n",
    "    plt.rcParams.update(parameters)\n",
    "    \n",
    "    plt.figure(figsize = (figsize_horz, figsize_vert))\n",
    "    \n",
    "    condition_true_data = pd.DataFrame()\n",
    "    condition_false_data = pd.DataFrame()\n",
    "      \n",
    "    if index_min > -1 and index_max > -1 and index_min < index_max:\n",
    "        condition_true_data = data.iloc[index_min:index_max].query(data[condition_column].isin(condition_values))\n",
    "        condition_false_data = data.iloc[index_min:index_max].query(~data[condition_column].isin(condition_values))\n",
    "    elif index_min > -1:\n",
    "        condition_true_data = data.iloc[index_min:].query(data[condition_column].isin(condition_values))\n",
    "        condition_false_data = data.iloc[index_min:].query(~data[condition_column].isin(condition_values))        \n",
    "    elif index_max > -1:\n",
    "        condition_true_data = data.iloc[:index_max].query(data[condition_column].isin(condition_values))\n",
    "        condition_false_data = data.iloc[:index_max].query(~data[condition_column].isin(condition_values))        \n",
    "    else:\n",
    "        condition_true_data = data[data[condition_column].isin(condition_values)]\n",
    "        condition_false_data = data[~data[condition_column].isin(condition_values)]\n",
    "        \n",
    "    plt.plot(condition_true_data.index, condition_true_data[target_column_name], marker=marker, linestyle=linestyle, label=data_names[0], color='red', markersize=marker_size)\n",
    "    plt.plot(condition_false_data.index, condition_false_data[target_column_name], marker=marker, linestyle=linestyle, label=data_names[1], markersize=marker_size)\n",
    "    plt.xticks(fontsize = tick_label_font_size)\n",
    "    plt.yticks(fontsize = tick_label_font_size)\n",
    "    \n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "    plt.legend(data_names, loc = legend_location)\n",
    "    \n",
    "    if x_axis_title is not None:\n",
    "        plt.xlabel(x_axis_title, fontsize = 15)\n",
    "    if y_axis_title is not None:\n",
    "        plt.ylabel(y_axis_title, fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly score 기반 threshould 도출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshould_deduction(data, target_column_name, graph = True):\n",
    "    # True일 경우 training_data\n",
    "    # False일 경우 data\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pickle\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    import joblib\n",
    "    try:\n",
    "        joblib.load(os.path.join(os.getcwd(), 'IF_training_model.pkl'))\n",
    "    except FileNotFoundError:\n",
    "        print('IF_training_model File does not exist.')\n",
    "        print('This need a learning model.')\n",
    "        print('Please training model establishment first.')\n",
    "    else:\n",
    "        input_data = data[target_column_name].values.reshape(-1,1)\n",
    "        training_model = joblib.load(os.path.join(os.getcwd(), 'IF_training_model.pkl'))\n",
    "        input_data_anomaly_score = training_model.decision_function(input_data)\n",
    "        input_data_outlier = training_model.predict(input_data)\n",
    "        temp_input = np.concatenate((input_data.reshape(-1,1), input_data_outlier.reshape(-1,1)), axis = 1)\n",
    "        result = pd.DataFrame(temp_input, columns = [target_column_name, 'outlier'])\n",
    "        if graph == True:\n",
    "            plt.figure(figsize = (20,12))\n",
    "            plt.scatter(input_data, input_data_anomaly_score, label = 'anomaly score')\n",
    "            plt.fill_between(input_data.T[0], np.min(input_data_anomaly_score), np.max(input_data_anomaly_score), where=input_data_outlier==-1, color='r', \n",
    "                             alpha=.3, label='outlier region')\n",
    "            plt.axvline(min(result.query('outlier == -1')[target_column_name]),  color = 'g', linestyle = 'dashed')\n",
    "            plt.text(min(result.query('outlier == -1')[target_column_name]), np.min(input_data_anomaly_score)-0.1, 'Threshold {:.4f}'.format(min(result.query('outlier == -1')[target_column_name])), color = 'r', fontsize = 12, rotation = 90)\n",
    "            plt.legend()\n",
    "            plt.ylabel('anomaly score', fontsize = 15)\n",
    "            plt.yticks(fontsize = 13)\n",
    "            plt.xlabel(target_column_name, fontsize = 15)\n",
    "            plt.xticks(fontsize = 13)\n",
    "        return min(result.query('outlier == -1')[target_column_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_threshould_deduction(data, graph = True, *target_column_name):\n",
    "    # True일 경우 training_data\n",
    "    # False일 경우 data\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pickle\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    import joblib\n",
    "    try:\n",
    "        joblib.load(os.path.join(os.getcwd(), 'IF_training_model.pkl'))\n",
    "    except FileNotFoundError:\n",
    "        print('IF_training_model File does not exist.')\n",
    "        print('This need a learning model.')\n",
    "        print('Please training model establishment first.')\n",
    "    else:\n",
    "        target_column_name = list(target_column_name)\n",
    "        training_model = joblib.load(os.path.join(os.getcwd(), 'IF_training_model.pkl'))\n",
    "        if len(target_column_name) == 1:\n",
    "            input_data = data[target_column_name].values.reshape(-1,1)\n",
    "            input_data_anomaly_score = training_model.decision_function(input_data)\n",
    "            input_data_outlier = training_model.predict(input_data)\n",
    "            temp_input = np.concatenate((input_data.reshape(-1,1), input_data_outlier.reshape(-1,1)), axis = 1)\n",
    "            result = pd.DataFrame(temp_input, columns = [target_column_name, 'outlier'])\n",
    "        else: \n",
    "            input_data = data[target_column_name].values\n",
    "            input_data_anomaly_score = training_model.decision_function(input_data)\n",
    "            input_data_outlier = training_model.predict(input_data)\n",
    "            result = data[target_column_name]\n",
    "            result['outlier'] = input_data_outlier\n",
    "        #return min(result.query('outlier == -1')[target_column_name])\n",
    "        \n",
    "        if graph == True:\n",
    "            fig = plt.figure(figsize = (20,12))\n",
    "            if len(target_column_name) == 1:\n",
    "                plt.figure(figsize = (20,12))\n",
    "                plt.scatter(input_data.T[idx], input_data_anomaly_score, label = 'anomaly score')\n",
    "                plt.fill_between(input_data.T[idx], np.min(input_data_anomaly_score), np.max(input_data_anomaly_score), where=input_data_outlier==-1, color='r', \n",
    "                                 alpha=.3, label='outlier region')\n",
    "                plt.axvline(min(result.query('outlier == -1')[target_column_name]),  color = 'g', linestyle = 'dashed')\n",
    "                plt.text(min(result.query('outlier == -1')[target_column_name]), np.min(input_data_anomaly_score)-0.2, 'Threshold {:.4f}'.format(min(result.query('outlier == -1')[target_column_name])), color = 'r', fontsize = 12, rotation = 90)\n",
    "                plt.legend()\n",
    "                plt.ylabel('anomaly score', fontsize = 15)\n",
    "                plt.yticks(fontsize = 13)\n",
    "                plt.xlabel(target_column_name, fontsize = 15)\n",
    "                plt.xticks(fontsize = 13)\n",
    "                \n",
    "            else:\n",
    "    \n",
    "                for idx in range(len(target_column_name)):\n",
    "                    globals()['ax_{}'.format(idx)] = fig.add_subplot(len(target_column_name), 1, idx+1)\n",
    "                    globals()['ax_{}'.format(idx)].scatter(input_data.T[idx], input_data_anomaly_score, label = 'anomaly score')\n",
    "                    globals()['ax_{}'.format(idx)].fill_between(input_data.T[idx], np.min(input_data_anomaly_score), np.max(input_data_anomaly_score), where=input_data_outlier==-1, color='r', \n",
    "                                     alpha=.5, label='outlier region')\n",
    "                    print(result.query('outlier == -1'))\n",
    "                    globals()['ax_{}'.format(idx)].axvline(min(result.query('outlier == -1')[target_column_name[idx]]),  color = 'g', linestyle = 'dashed')\n",
    "                    globals()['ax_{}'.format(idx)].text(min(result.query('outlier == -1')[target_column_name[idx]]), np.min(input_data_anomaly_score)-0.08, 'Threshold', fontsize = 12, color = 'g')\n",
    "                    # plt.legend()\n",
    "                    globals()['ax_{}'.format(idx)].set_ylabel('anomaly score', fontsize = 15)\n",
    "                    #plt.yticks(fontsize = 13)\n",
    "                    globals()['ax_{}'.format(idx)].set_xlabel('{}'.format(target_column_name[idx]), fontsize = 15)\n",
    "                    \n",
    "            return min(result.query('outlier == -1')[target_column_name])\n",
    "\n",
    "                    #plt.xticks(fontsize = 13)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshould를 적용한 시각화 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot_with_threshould(data, target_column_name, condition_column, condition_values, data_names = ('condition is true','condition is false'),\n",
    "                                           index_min = -1, index_max = -1, title = None, title_font_size=20,\n",
    "                                           x_axis_title = None, y_axis_title = None, axes_title_font_size=18,\n",
    "                                           tick_label_font_size=15, figsize_horz=20, figsize_vert=10,\n",
    "                                           legend_font_size=15, legend_location=\"lower right\", linestyle = 'none',\n",
    "                                           marker='.', marker_size=10):\n",
    "    \n",
    "    try:\n",
    "        joblib.load(os.path.join(os.getcwd(), 'IF_training_model.pkl'))\n",
    "    except FileNotFoundError:\n",
    "        print('IF_training_model File does not exist.')\n",
    "        print('This need a learning model.')\n",
    "        print('Please training model establishment first.')\n",
    "    else:\n",
    "        parameters = { 'figure.titlesize':title_font_size, 'axes.titlesize': axes_title_font_size,\n",
    "                      'axes.labelsize' : tick_label_font_size, 'legend.fontsize' : legend_font_size }\n",
    "        plt.rcParams.update(parameters)\n",
    "\n",
    "        plt.figure(figsize = (figsize_horz, figsize_vert))\n",
    "\n",
    "        input_data = data[target_column_name].values.reshape(-1,1)\n",
    "        training_model = joblib.load(os.path.join(os.getcwd(), 'IF_training_model.pkl'))\n",
    "        input_data_anomaly_score = training_model.decision_function(input_data)\n",
    "        input_data_outlier = training_model.predict(input_data)\n",
    "        temp_input = np.concatenate((input_data.reshape(-1,1), input_data_outlier.reshape(-1,1)), axis = 1)\n",
    "\n",
    "        condition_true_data = pd.DataFrame()\n",
    "        condition_false_data = pd.DataFrame()\n",
    "\n",
    "        if index_min > -1 and index_max > -1 and index_min < index_max:\n",
    "            condition_true_data = data.iloc[index_min:index_max].query(data[condition_column].isin(condition_values))\n",
    "            condition_false_data = data.iloc[index_min:index_max].query(~data[condition_column].isin(condition_values))\n",
    "        elif index_min > -1:\n",
    "            condition_true_data = data.iloc[index_min:].query(data[condition_column].isin(condition_values))\n",
    "            condition_false_data = data.iloc[index_min:].query(~data[condition_column].isin(condition_values))        \n",
    "        elif index_max > -1:\n",
    "            condition_true_data = data.iloc[:index_max].query(data[condition_column].isin(condition_values))\n",
    "            condition_false_data = data.iloc[:index_max].query(~data[condition_column].isin(condition_values))        \n",
    "        else:\n",
    "            condition_true_data = data[data[condition_column].isin(condition_values)]\n",
    "            condition_false_data = data[~data[condition_column].isin(condition_values)]\n",
    "\n",
    "        plt.plot(condition_true_data.index, condition_true_data[target_column_name], marker=marker, linestyle=linestyle, label=data_names[0], color='red', markersize=marker_size)\n",
    "        plt.plot(condition_false_data.index, condition_false_data[target_column_name], marker=marker, linestyle=linestyle, label=data_names[1], markersize=marker_size)\n",
    "        plt.axhline(threshould_deduction(data, target_column_name, graph = False), color = 'r', linestyle = 'dashed')\n",
    "        plt.text(-290, threshould_deduction(data, target_column_name, graph = False)-15, 'Threshold {:.4f}'.format(min(pd.DataFrame(temp_input, columns = [target_column_name, 'outlier']).query('outlier == -1')[target_column_name])), color = 'r', fontsize = 12)\n",
    "\n",
    "        if title is not None:\n",
    "            plt.title(title)\n",
    "\n",
    "        plt.legend(data_names, loc = legend_location)\n",
    "        plt.xticks(fontsize = 15)\n",
    "        plt.yticks(fontsize = 15)\n",
    "\n",
    "        if x_axis_title is not None:\n",
    "            plt.xlabel(x_axis_title, fontsize = 15)\n",
    "        if y_axis_title is not None:\n",
    "            plt.ylabel(y_axis_title, fontsize = 15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
